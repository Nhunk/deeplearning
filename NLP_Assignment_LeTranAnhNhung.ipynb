{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Level 1: Basic Concepts**\n",
        "\n",
        "### **Q1. Simple Word Tokenization**\n",
        " Write a Python function that tokenizes a sentence into words using:\n",
        "\n",
        " a) split() method\n",
        "\n",
        " b) Regular expressions\n",
        "\n",
        " c) NLTK's word_tokenize()\n"
      ],
      "metadata": {
        "id": "CkFaJzZvqm2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WF3zz5eqigm",
        "outputId": "b941eee3-0055-4e75-e3c9-fd7fa69a01e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split():  ['Natural', 'Language', 'Processing', 'is', 'amazing!']\n"
          ]
        }
      ],
      "source": [
        "input1 = \"Natural Language Processing is amazing!\"\n",
        "\n",
        "#split() method\n",
        "def tokenize_split(text):\n",
        "  return text.split()\n",
        "print(\"Split(): \",tokenize_split(input1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Regular expressions\n",
        "import re\n",
        "def tokenize_regex(text):\n",
        "  return re.findall(r'\\b\\w+\\b', text)\n",
        "print(\"Regular expressions: \",tokenize_regex(input1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRRUUponQbBv",
        "outputId": "2b593a47-f8bc-4574-9de5-4c0b928bcc17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular expressions:  ['Natural', 'Language', 'Processing', 'is', 'amazing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk's word_tokenize\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def tokenize_nltk(text):\n",
        "  return word_tokenize(text)\n",
        "print(\"NLTK's word_tokenize: \",tokenize_nltk(input1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkOna4KPQ8D4",
        "outputId": "754cd8e2-dec9-478a-de4d-9b741a430d55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK's word_tokenize:  ['Natural', 'Language', 'Processing', 'is', 'amazing', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q2. Character Tokenization**\n",
        " Create a function that converts text into character-level tokens, including spaces"
      ],
      "metadata": {
        "id": "WHo-A8aCuBD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def char_tokenize(text: str):\n",
        "  return list(text)\n",
        "\n",
        "print(char_tokenize(\"NLP\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCbQ7wNXtSOF",
        "outputId": "84d0f277-6e84-455a-c619-6e65fb6fb8ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['N', 'L', 'P']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q3. Whitespace vs Punctuation**\n",
        " Compare the outputs of whitespace tokenization vs. word tokenization that handles\n",
        " punctuation."
      ],
      "metadata": {
        "id": "VBct1oTxuTK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input3= \"Hello, world! How's it going?\"\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "  return text.split()\n",
        "print(\"whitespace_tokenize: \",whitespace_tokenize(input3))\n",
        "\n",
        "def word_tokenize(text):\n",
        "  return nltk.word_tokenize(text)\n",
        "print(\"word_tokenize: \",word_tokenize(input3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvu_TYn4uNmk",
        "outputId": "0d3d0569-93fb-4973-862d-cc48f08fd6f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whitespace_tokenize:  ['Hello,', 'world!', \"How's\", 'it', 'going?']\n",
            "word_tokenize:  ['Hello', ',', 'world', '!', 'How', \"'s\", 'it', 'going', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Level 2: Library-Based Tokenization**\n",
        " ### **Q4. NLTK Tokenizers**\n",
        " Write code to tokenize the following text using:"
      ],
      "metadata": {
        "id": "I2MruRNwv_Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input4 = \"Dr. Smith works at A.I. Corp. She loves NLP! Does she?\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize,RegexpTokenizer\n",
        "\n",
        "words = word_tokenize(input4)\n",
        "print(\"Word Tokenization:\", words)\n",
        "\n",
        "sentences = sent_tokenize(input4)\n",
        "print(\"Sentence Tokenization:\", sentences)\n",
        "\n",
        "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "regex_tokens = regexp_tokenizer.tokenize(input4)\n",
        "print(\"Regexp Tokenization:\", regex_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5_FH26gvAxy",
        "outputId": "9328ee52-8b88-40c6-9fee-14b63485b597"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization: ['Dr.', 'Smith', 'works', 'at', 'A.I', '.', 'Corp.', 'She', 'loves', 'NLP', '!', 'Does', 'she', '?']\n",
            "Sentence Tokenization: ['Dr. Smith works at A.I.', 'Corp. She loves NLP!', 'Does she?']\n",
            "Regexp Tokenization: ['Dr', 'Smith', 'works', 'at', 'A', 'I', 'Corp', 'She', 'loves', 'NLP', 'Does', 'she']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q5. spaCy Tokenization**\n",
        " Use spaCy to tokenize and identify:\n",
        " Tokens\n",
        " Lemmas\n",
        " Part-of-speech tags"
      ],
      "metadata": {
        "id": "ERISooc3x0GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input5 =  \"The runners are running faster than expected.\"\n",
        "\n",
        "import spacy\n",
        "# Load English small model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# NLP pipeline\n",
        "doc = nlp(input5)\n",
        "\n",
        "print(f\"{'Token':<12}{'Lemma':<12}{'POS':<8}\")\n",
        "print(\"-\"*32)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<12}{token.lemma_:<12}{token.pos_:<8}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4EqfPgMw4bm",
        "outputId": "1fd734b7-c951-4aa1-eab8-4ad624e925b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token       Lemma       POS     \n",
            "--------------------------------\n",
            "The         the         DET     \n",
            "runners     runner      NOUN    \n",
            "are         be          AUX     \n",
            "running     run         VERB    \n",
            "faster      fast        ADV     \n",
            "than        than        SCONJ   \n",
            "expected    expect      VERB    \n",
            ".           .           PUNCT   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### **Q6. Handling Contractions**\n",
        " Write a function to properly tokenize contractions"
      ],
      "metadata": {
        "id": "uFI4WQpBymf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input6 =  \"I'm doesn't can't won't\"\n",
        "\n",
        "#!pip install contractions\n",
        "import contractions\n",
        "\n",
        "# Expand contractions\n",
        "def expand_contractions(text):\n",
        "    expanded = contractions.fix(text)\n",
        "    tokens = word_tokenize(expanded)\n",
        "    return tokens\n",
        "\n",
        "tokens = expand_contractions(input6)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5qxrlHjyXD8",
        "outputId": "bb802124-b178-426a-d7f7-6cccbbb272b1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'does', 'not', 'can', 'not', 'will', 'not']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Level 3: Subword Tokenization**\n",
        " ### **Q7. Byte Pair Encoding (BPE)**\n",
        " Implement a simple BPE tokenizer from scratch that:\n",
        " - Starts with character-level tokens\n",
        " - Merges the most frequent pair iteratively"
      ],
      "metadata": {
        "id": "n8Uov1Xl1SiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input7 = [\"low\", \"lower\", \"lowest\", \"newer\", \"wider\"]\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i+1])] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "    merged_vocab = {}\n",
        "    bigram = ' '.join(pair)\n",
        "    replacement = ''.join(pair)\n",
        "    for word in vocab:\n",
        "        new_word = word.replace(bigram, replacement)\n",
        "        merged_vocab[new_word] = vocab[word]\n",
        "    return merged_vocab\n",
        "\n",
        "vocab = {' '.join(list(word)) + ' </w>': 1 for word in input7}\n",
        "\n",
        "print(\"Initial Vocabulary:\")\n",
        "for k in vocab:\n",
        "    print(k)\n",
        "\n",
        "num_merges = 5\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print(f\"\\nMerge {i+1}: {best}\")\n",
        "    for k in vocab:\n",
        "        print(k)"
      ],
      "metadata": {
        "id": "-SZF3kvC2t3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452f118c-81f9-44e7-f0ad-49ed3fdfb008"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Vocabulary:\n",
            "l o w </w>\n",
            "l o w e r </w>\n",
            "l o w e s t </w>\n",
            "n e w e r </w>\n",
            "w i d e r </w>\n",
            "\n",
            "Merge 1: ('l', 'o')\n",
            "lo w </w>\n",
            "lo w e r </w>\n",
            "lo w e s t </w>\n",
            "n e w e r </w>\n",
            "w i d e r </w>\n",
            "\n",
            "Merge 2: ('lo', 'w')\n",
            "low </w>\n",
            "low e r </w>\n",
            "low e s t </w>\n",
            "n e w e r </w>\n",
            "w i d e r </w>\n",
            "\n",
            "Merge 3: ('e', 'r')\n",
            "low </w>\n",
            "low er </w>\n",
            "low e s t </w>\n",
            "n e w er </w>\n",
            "w i d er </w>\n",
            "\n",
            "Merge 4: ('er', '</w>')\n",
            "low </w>\n",
            "low er</w>\n",
            "low e s t </w>\n",
            "n e w er</w>\n",
            "w i d er</w>\n",
            "\n",
            "Merge 5: ('low', '</w>')\n",
            "low</w>\n",
            "low er</w>\n",
            "low e s t </w>\n",
            "n e w er</w>\n",
            "w i d er</w>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q8. Hugging Face Tokenizers**\n",
        " Use transformers library to tokenize with:\n",
        "- BERT tokenizer\n",
        " - GPT-2 tokenizer\n",
        " - T5 tokenizer\n",
        " \"Tokenization is fundamental!\"\n",
        "\n",
        " Text:\"Tokenization is fundamental!\"\n",
        "\n",
        " Task: Compare vocabulary handling and special tokens\n"
      ],
      "metadata": {
        "id": "d_g9n5uO_w8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, GPT2Tokenizer, T5Tokenizer\n",
        "\n",
        "input8 = \"Tokenization is fundamental!\"\n",
        "\n",
        "def compare_tokenizers(text):\n",
        "    result = {}\n",
        "\n",
        "    # BERT\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    bert_tokens = bert_tokenizer.tokenize(text)\n",
        "    bert_ids = bert_tokenizer.encode(text)\n",
        "    bert_special = {'cls_token': bert_tokenizer.cls_token,\n",
        "                    'sep_token': bert_tokenizer.sep_token,\n",
        "                    'pad_token': bert_tokenizer.pad_token}\n",
        "    result['BERT'] = {'tokens': bert_tokens, 'ids': bert_ids, 'special_tokens': bert_special}\n",
        "\n",
        "    # GPT-2\n",
        "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    gpt2_tokens = gpt2_tokenizer.tokenize(text)\n",
        "    gpt2_ids = gpt2_tokenizer.encode(text)\n",
        "    # GPT-2 không có CLS/SEP\n",
        "    gpt2_special = {'cls_token': gpt2_tokenizer.cls_token,\n",
        "                    'sep_token': gpt2_tokenizer.sep_token,\n",
        "                    'pad_token': gpt2_tokenizer.pad_token}\n",
        "    result['GPT-2'] = {'tokens': gpt2_tokens, 'ids': gpt2_ids, 'special_tokens': gpt2_special}\n",
        "\n",
        "    # T5\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "    t5_tokens = t5_tokenizer.tokenize(text)\n",
        "    t5_ids = t5_tokenizer.encode(text)\n",
        "    t5_special = {'pad_token': t5_tokenizer.pad_token,\n",
        "                   'eos_token': t5_tokenizer.eos_token,\n",
        "                   'unk_token': t5_tokenizer.unk_token}\n",
        "    result['T5'] = {'tokens': t5_tokens, 'ids': t5_ids, 'special_tokens': t5_special}\n",
        "\n",
        "    return result\n",
        "\n",
        "res = compare_tokenizers(input8)\n",
        "for model, info in res.items():\n",
        "    print(f\"\\n--- {model} ---\")\n",
        "    print(\"Tokens:\", info['tokens'])\n",
        "    print(\"Token IDs:\", info['ids'])\n",
        "    print(\"Special tokens:\", info['special_tokens'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlQeD68hkel0",
        "outputId": "eba5c48a-3efd-42a1-9c26-0d3d62c39f31"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- BERT ---\n",
            "Tokens: ['token', '##ization', 'is', 'fundamental', '!']\n",
            "Token IDs: [101, 19204, 3989, 2003, 8050, 999, 102]\n",
            "Special tokens: {'cls_token': '[CLS]', 'sep_token': '[SEP]', 'pad_token': '[PAD]'}\n",
            "\n",
            "--- GPT-2 ---\n",
            "Tokens: ['Token', 'ization', 'Ġis', 'Ġfundamental', '!']\n",
            "Token IDs: [30642, 1634, 318, 7531, 0]\n",
            "Special tokens: {'cls_token': None, 'sep_token': None, 'pad_token': None}\n",
            "\n",
            "--- T5 ---\n",
            "Tokens: ['▁To', 'ken', 'ization', '▁is', '▁fundamental', '!']\n",
            "Token IDs: [304, 2217, 1707, 19, 4431, 55, 1]\n",
            "Special tokens: {'pad_token': '<pad>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q9. WordPiece Tokenization**\n",
        " Tokenize the following using BERT's WordPiece:\n",
        "\n",
        " Input: \"unhappiness\"\n",
        "\n",
        " Expected:['un', '##hap', '##pi', '##ness']"
      ],
      "metadata": {
        "id": "wDzFH_hEADPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input9 = \"unhappiness\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer.tokenize(input9)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2SU6FmmlVQ2",
        "outputId": "606c87f8-5ff7-45e6-cebc-8bea12064dba"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['un', '##ha', '##pp', '##iness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Level 4: Advanced Challenges**\n",
        " ### **Q10. Custom Tokenizer Class**\n",
        " Create a\n",
        "CustomTokenizer\n",
        " class with methods:\n",
        " - fit(corpus) - builds vocabulary\n",
        " - tokenize(text)- tokenizes\n",
        " - text vocab_size() - returns vocabulary size\n",
        " - encode(text) - converts to token IDs\n",
        " - decode(ids) - converts IDs back to text"
      ],
      "metadata": {
        "id": "GOGzGihAKwA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}      # token -> id\n",
        "        self.id2token = {}   # id -> token\n",
        "\n",
        "    def fit(self, corpus):\n",
        "        token_freq = defaultdict(int)\n",
        "        for text in corpus:\n",
        "            tokens = self.tokenize(text)\n",
        "            for t in tokens:\n",
        "                token_freq[t] += 1\n",
        "\n",
        "        # Sort tokens by frequency (optional) and assign IDs\n",
        "        self.vocab = {token: idx for idx, token in enumerate(token_freq.keys())}\n",
        "        self.id2token = {idx: token for token, idx in self.vocab.items()}\n",
        "\n",
        "    #  Tokenize text\n",
        "    def tokenize(self, text):\n",
        "        tokens = text.lower().replace('.', ' .').replace(',', ' ,').split()\n",
        "        return tokens\n",
        "\n",
        "    # Vocabulary size\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    # Encode text to token IDs\n",
        "    def encode(self, text):\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocab[t] for t in tokens if t in self.vocab]\n",
        "\n",
        "    # Decode IDs to text\n",
        "    def decode(self, ids):\n",
        "        tokens = [self.id2token[i] for i in ids if i in self.id2token]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "corpus = [\n",
        "    \"Tokenization is fundamental.\",\n",
        "    \"Custom tokenizer class example.\"\n",
        "]\n",
        "\n",
        "tokenizer = CustomTokenizer()\n",
        "tokenizer.fit(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", tokenizer.vocab)\n",
        "print(\"Vocabulary size:\", tokenizer.vocab_size())\n",
        "\n",
        "text = \"Tokenization is example.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(\"Encoded IDs:\", ids)\n",
        "print(\"Decoded text:\", tokenizer.decode(ids))\n"
      ],
      "metadata": {
        "id": "vNGGKT9IBRWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cfb0262-01f9-411b-b885-b921d9357706"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'tokenization': 0, 'is': 1, 'fundamental': 2, '.': 3, 'custom': 4, 'tokenizer': 5, 'class': 6, 'example': 7}\n",
            "Vocabulary size: 8\n",
            "Encoded IDs: [0, 1, 7, 3]\n",
            "Decoded text: tokenization is example .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q11. Handling Multiple Languages**\n",
        " Write a tokenizer that correctly handles:\n",
        "\n",
        "English: \"Hello world\"\n",
        "\n",
        "Chinese: \"你好世界\"\n",
        "\n",
        "Mixed: \"Hello 世界\"\n",
        "\n",
        "Hint: Use language-specific tokenizers\n"
      ],
      "metadata": {
        "id": "-lR8GNKELNqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "class MultiLangTokenizer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = []\n",
        "        # Tách English bằng regex\n",
        "        english_words = re.findall(r'[a-zA-Z]+', text)\n",
        "        # Tách Chinese bằng jieba\n",
        "        chinese_words = jieba.lcut(text)\n",
        "\n",
        "        i = 0\n",
        "        while i < len(text):\n",
        "            c = text[i]\n",
        "            if re.match(r'[a-zA-Z]', c):\n",
        "                word = c\n",
        "                i += 1\n",
        "                while i < len(text) and re.match(r'[a-zA-Z]', text[i]):\n",
        "                    word += text[i]\n",
        "                    i += 1\n",
        "                tokens.append(word)\n",
        "            elif re.match(r'[\\u4e00-\\u9fff]', c):\n",
        "                word = ''\n",
        "                for seg in chinese_words:\n",
        "                    if seg in text[i:]:\n",
        "                        word = seg\n",
        "                        i += len(seg)\n",
        "                        tokens.append(word)\n",
        "                        break\n",
        "            else:\n",
        "                # Dấu câu hoặc space\n",
        "                if not c.isspace():\n",
        "                    tokens.append(c)\n",
        "                i += 1\n",
        "        return tokens\n",
        "\n",
        "# Example usage\n",
        "tokenizer = MultiLangTokenizer()\n",
        "\n",
        "input11 = [\n",
        "    \"Hello world\",     # English\n",
        "    \"你好世界\",         # Chinese\n",
        "    \"Hello 世界\"       # Mixed\n",
        "]\n",
        "\n",
        "for t in input11:\n",
        "    print(f\"Input: {t}\")\n",
        "    print(\"Tokens:\", tokenizer.tokenize(t))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZKQBX5To7jx",
        "outputId": "1fd55890-0efe-4e4f-b68c-8bb0c6ff8962"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Hello world\n",
            "Tokens: ['Hello', 'world']\n",
            "\n",
            "Input: 你好世界\n",
            "Tokens: ['你好', '世界']\n",
            "\n",
            "Input: Hello 世界\n",
            "Tokens: ['Hello', '世界']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q12. Token Normalization**\n",
        " Implement a pipeline that:\n",
        " 1. Converts to lowercase\n",
        " 2. Removes punctuation\n",
        " 3. Tokenizes\n",
        " 4. Removes stopwords\n",
        " 5. Applies stemming/lemmatization\n",
        "\n",
        " Input: \"The quick brown foxes are jumping over the lazy dogs!\""
      ],
      "metadata": {
        "id": "M1LKi3MYLbNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diFvjy5XqdLA",
        "outputId": "ce77cb39-1c07-4c89-bac7-596a5bf18185"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def normalize_text_nltk(text):\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 3. Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "    # 5. Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(t) for t in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "input12 = \"The quick brown foxes are jumping over the lazy dogs!\"\n",
        "normalized_tokens = normalize_text_nltk(input12)\n",
        "print(normalized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdQh22daqOvI",
        "outputId": "585bb9fe-cc51-492f-f6bd-e879a5e39e0c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Level 5: Performance & Edge Cases**\n",
        " ### **Q13. Tokenization Speed Benchmark**\n",
        " Compare the speed of different tokenizers on a large corpus (100K sentences):\n",
        "\n",
        " - str.split()\n",
        " - NLTK\n",
        " - spaCy\n",
        " - Hugging Face Fast Tokenizers"
      ],
      "metadata": {
        "id": "iGa1AgLZLmVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "corpus_size = 100000  # 100K sentences\n",
        "words = [\"tokenization\", \"is\", \"fundamental\", \"for\", \"NLP\", \"example\", \"model\",\n",
        "         \"data\", \"learning\", \"python\", \"language\", \"text\", \"analysis\"]\n",
        "corpus = [\" \".join(random.choices(words, k=10)) for _ in range(corpus_size)]\n",
        "print(\"Corpus created with\", len(corpus), \"sentences.\")\n",
        "\n",
        "#str.split()\n",
        "start = time.time()\n",
        "split_tokens = [sentence.split() for sentence in corpus]\n",
        "end = time.time()\n",
        "print(\"str.split() time:\", round(end - start, 3), \"s\")\n",
        "\n",
        "#NLTK\n",
        "start = time.time()\n",
        "nltk_tokens = [word_tokenize(sentence) for sentence in corpus]\n",
        "end = time.time()\n",
        "print(\"NLTK time:\", round(end - start, 3), \"s\")\n",
        "\n",
        "#spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "start = time.time()\n",
        "spacy_tokens = [[token.text for token in doc] for doc in nlp.pipe(corpus, batch_size=1000)]\n",
        "end = time.time()\n",
        "print(\"spaCy time:\", round(end - start, 3), \"s\")\n",
        "\n",
        "#Hugging Face Fast Tokenizer\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
        "start = time.time()\n",
        "hf_tokens = [hf_tokenizer.tokenize(sentence) for sentence in corpus]\n",
        "end = time.time()\n",
        "print(\"Hugging Face Fast Tokenizer time:\", round(end - start, 3), \"s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRr4RU4bpl2o",
        "outputId": "ee671807-0dcc-40b5-8c60-9843d22cd174"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus created with 100000 sentences.\n",
            "str.split() time: 0.953 s\n",
            "NLTK time: 9.077 s\n",
            "spaCy time: 86.648 s\n",
            "Hugging Face Fast Tokenizer time: 12.167 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q14. Vocabulary Size Optimization**\n",
        " Given a corpus, implement a function that:\n",
        " - Creates vocabularies of different sizes (1K, 5K, 10K, 50K)\n",
        " - Calculates out-of-vocabulary (OOV) rate for each\n",
        " - Plots vocab size vs. OOV rate"
      ],
      "metadata": {
        "id": "B6ZBHBG_Lwgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corpus_size = 50000  # 50K sentences\n",
        "# Tạo 5000 từ ngẫu nhiên\n",
        "vocab_pool = [''.join(random.choices(string.ascii_lowercase, k=5)) for _ in range(5000)]\n",
        "\n",
        "# Tạo corpus 50K câu, mỗi câu 10 từ\n",
        "corpus = [\" \".join(random.choices(vocab_pool, k=10)) for _ in range(50000)]\n",
        "\n",
        "tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "# Flatten list of tokens\n",
        "all_tokens = [token for sentence in tokenized_corpus for token in sentence]\n",
        "\n",
        "# Count frequency\n",
        "token_freq = Counter(all_tokens)\n",
        "\n",
        "#3. Function: calculate OOV rate\n",
        "def oov_rate(corpus_tokens, vocab_size):\n",
        "    # Get top vocab_size tokens\n",
        "    vocab = set([token for token, freq in token_freq.most_common(vocab_size)])\n",
        "    total_tokens = sum(len(sentence) for sentence in corpus_tokens)\n",
        "    oov_count = sum(1 for sentence in corpus_tokens for token in sentence if token not in vocab)\n",
        "    return oov_count / total_tokens  # OOV rate\n",
        "\n",
        "#4. Test for different vocab sizes\n",
        "vocab_sizes = [1000, 5000, 10000, 50000]\n",
        "oov_rates = []\n",
        "\n",
        "for vs in vocab_sizes:\n",
        "    rate = oov_rate(tokenized_corpus, vs)\n",
        "    oov_rates.append(rate)\n",
        "    print(f\"Vocab size: {vs}, OOV rate: {rate:.3f}\")\n",
        "\n",
        "#5. Plot vocab size vs. OOV rate\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(vocab_sizes, oov_rates, marker='o')\n",
        "plt.xlabel(\"Vocabulary Size\")\n",
        "plt.ylabel(\"OOV Rate\")\n",
        "plt.title(\"Vocabulary Size vs. OOV Rate\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "pmfWUCCjrdD5",
        "outputId": "9a65d70f-b7be-427a-b78c-0944502bb695"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 1000, OOV rate: 0.771\n",
            "Vocab size: 5000, OOV rate: 0.000\n",
            "Vocab size: 10000, OOV rate: 0.000\n",
            "Vocab size: 50000, OOV rate: 0.000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q15. Special Tokens Handling**\n",
        " Create a tokenizer that properly handles:\n",
        "\n",
        "-  URLs:  \"Visit https://example.com\"\n",
        "- Emails: \"contact@email.com\"\n",
        "- Hashtags: \"#NLP #DeepLearning\"\n",
        "- Mentions: \"@username\"\n",
        "- Emojis: \"I love NLP \""
      ],
      "metadata": {
        "id": "NsS_A1H3L3Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5MpJnV_s8t5",
        "outputId": "8a5b5c60-5a49-4a2c-b5db-e196d49932c5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.2/608.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "\n",
        "class SocialMediaTokenizer:\n",
        "    def __init__(self):\n",
        "        self.url_pattern = r'https?://\\S+'\n",
        "        self.email_pattern = r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b'\n",
        "        self.hashtag_pattern = r'#\\w+'\n",
        "        self.mention_pattern = r'@\\w+'\n",
        "        self.emoji_pattern = re.compile(\"|\".join(map(re.escape, emoji.EMOJI_DATA.keys())))\n",
        "        self.word_pattern = r'\\b\\w+\\b'\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = []\n",
        "\n",
        "        for pattern in [self.url_pattern, self.email_pattern, self.hashtag_pattern, self.mention_pattern, self.emoji_pattern]:\n",
        "            matches = re.findall(pattern, text)\n",
        "            for m in matches:\n",
        "                tokens.append(m)\n",
        "                text = text.replace(m, ' ')\n",
        "\n",
        "        # Remaining words\n",
        "        words = re.findall(self.word_pattern, text)\n",
        "        tokens.extend(words)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "#Example Usage\n",
        "tokenizer = SocialMediaTokenizer()\n",
        "\n",
        "texts = [\n",
        "    \"Visit https://example.com\",\n",
        "    \"contact@email.com\",\n",
        "    \"#NLP #DeepLearning\",\n",
        "    \"@username\",\n",
        "    \"I love NLP ❤️\"\n",
        "]\n",
        "\n",
        "for t in texts:\n",
        "    print(f\"Input: {t}\")\n",
        "    print(\"Tokens:\", tokenizer.tokenize(t))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVITU5xTsp48",
        "outputId": "5d27b6e1-bf81-4775-d4f1-84da0bd4e1fd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Visit https://example.com\n",
            "Tokens: ['https://example.com', 'Visit']\n",
            "\n",
            "Input: contact@email.com\n",
            "Tokens: ['contact@email.com']\n",
            "\n",
            "Input: #NLP #DeepLearning\n",
            "Tokens: ['#NLP', '#DeepLearning']\n",
            "\n",
            "Input: @username\n",
            "Tokens: ['@username']\n",
            "\n",
            "Input: I love NLP ❤️\n",
            "Tokens: ['❤️', 'I', 'love', 'NLP']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bonus: Real-World Application**\n",
        " ### **Q16. Sentiment Analysis Preprocessing**\n",
        " Build a complete tokenization pipeline for sentiment analysis:\n",
        " - Clean HTML tags\n",
        "- Handle negations (n't, not)\n",
        "- Preserve emoticons\n",
        "- Create bigrams for important phrases\n",
        "- Pad sequences to fixed length\n",
        "\n",
        " *Test on: Amazon reviews dataset*"
      ],
      "metadata": {
        "id": "PHg0pxwwNKqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "class SentimentPre:\n",
        "    def __init__(self, sequence_length=20):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    #Clean HTML\n",
        "    def clean_html(self, text):\n",
        "        return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    #Handle negations\n",
        "    def handle_negations(self, tokens):\n",
        "        # Convert \"isn't\" -> \"is not\"\n",
        "        neg_pattern = re.compile(r\"(\\w+)n't\\b\")\n",
        "        new_tokens = []\n",
        "        for t in tokens:\n",
        "            match = neg_pattern.match(t)\n",
        "            if match:\n",
        "                new_tokens.extend([match.group(1), 'not'])\n",
        "            else:\n",
        "                new_tokens.append(t)\n",
        "        return new_tokens\n",
        "\n",
        "    #Preserve emoticons\n",
        "    def preserve_emoticons(self, text):\n",
        "        # Simple regex for emoticons\n",
        "        emoticon_pattern = r'[:;=8][\\-o\\*\\']?[\\)\\]\\(\\[dDpP/:}{@|\\\\]'\n",
        "        emoticons = re.findall(emoticon_pattern, text)\n",
        "        return emoticons\n",
        "\n",
        "    #Tokenize text\n",
        "    def tokenize(self, text):\n",
        "        text = self.clean_html(text)\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [t for t in tokens if t not in self.stop_words]\n",
        "        tokens = self.handle_negations(tokens)\n",
        "        return tokens\n",
        "\n",
        "    #Create bigrams\n",
        "    def create_bigrams(self, tokens):\n",
        "        bigram_list = list(ngrams(tokens, 2))\n",
        "        # Represent bigrams as joined strings\n",
        "        bigram_tokens = [\"_\".join(b) for b in bigram_list]\n",
        "        return tokens + bigram_tokens\n",
        "\n",
        "    #Pad sequences\n",
        "    def pad_sequence(self, tokens):\n",
        "        if len(tokens) > self.sequence_length:\n",
        "            return tokens[:self.sequence_length]\n",
        "        else:\n",
        "            return tokens + [\"<PAD>\"] * (self.sequence_length - len(tokens))\n",
        "\n",
        "    #Full preprocessing\n",
        "    def preprocess(self, text):\n",
        "        emoticons = self.preserve_emoticons(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        tokens += emoticons  # Add emoticons at the end\n",
        "        tokens = self.create_bigrams(tokens)\n",
        "        tokens = self.pad_sequence(tokens)\n",
        "        return tokens\n",
        "\n",
        "preprocessor = SentimentPre(sequence_length=15)\n",
        "\n",
        "sample_review = \"<p>I love this product! It's amazing :) but the battery isn't great.</p>\"\n",
        "\n",
        "processed_tokens = preprocessor.preprocess(sample_review)\n",
        "print(\"Processed Tokens:\")\n",
        "print(processed_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "ohRbrz2QNV2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cef38af-9b5c-4baa-c417-4efd9eeceeb5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Tokens:\n",
            "['love', 'product', '!', \"'s\", 'amazing', ':', ')', 'battery', \"n't\", 'great', '.', ':)', 'love_product', 'product_!', \"!_'s\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q17. Build a Tokenizer Comparison Tool**\n",
        "\n",
        "Create a function that compares multiple tokenizers side-by-side:\n",
        "\n",
        "```python\n",
        "def compare_tokenizers(text, tokenizers_list):\n",
        "    \"\"\"\n",
        "    Compare different tokenization approaches\n",
        "    Returns: DataFrame with columns [Tokenizer, Tokens, Count, Time]\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "8slui-qYBU1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compare_tokenizers(text, tokenizers_list):\n",
        "    \"\"\"\n",
        "    Compare different tokenization approaches\n",
        "    Returns: DataFrame with columns [Tokenizer, Tokens, Count, Time]\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Load spaCy tokenizer if needed\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "    # Load Hugging Face tokenizers if needed\n",
        "    hf_tokenizers = {}\n",
        "\n",
        "    for name in tokenizers_list:\n",
        "        start = time.time()\n",
        "\n",
        "        if name == \"str_split\":\n",
        "            tokens = text.split()\n",
        "        elif name == \"nltk\":\n",
        "            tokens = word_tokenize(text)\n",
        "        elif name == \"spacy\":\n",
        "            tokens = [token.text for token in nlp(text)]\n",
        "        elif name.startswith(\"hf_\"):\n",
        "            model_name = name.split(\"_\",1)[1]\n",
        "            if model_name not in hf_tokenizers:\n",
        "                hf_tokenizers[model_name] = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "            tokens = hf_tokenizers[model_name].tokenize(text)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown tokenizer: {name}\")\n",
        "\n",
        "        end = time.time()\n",
        "        results.append({\n",
        "            \"Tokenizer\": name,\n",
        "            \"Tokens\": tokens,\n",
        "            \"Count\": len(tokens),\n",
        "            \"Time\": round(end - start, 3)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    return df\n",
        "\n",
        "text = \"I love NLP! Visit https://example.com or contact me at email@example.com 😃 #NLP\"\n",
        "\n",
        "tokenizers_list = [\"str_split\", \"nltk\", \"spacy\", \"hf_bert-base-uncased\", \"hf_gpt2\"]\n",
        "\n",
        "df = compare_tokenizers(text, tokenizers_list)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "ZrXPPP7JAz1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17692158-a6bf-402f-d46a-4e360cab27af"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Tokenizer                                             Tokens  \\\n",
            "0             str_split  [I, love, NLP!, Visit, https://example.com, or...   \n",
            "1                  nltk  [I, love, NLP, !, Visit, https, :, //example.c...   \n",
            "2                 spacy  [I, love, NLP, !, Visit, https://example.com, ...   \n",
            "3  hf_bert-base-uncased  [i, love, nl, ##p, !, visit, https, :, /, /, e...   \n",
            "4               hf_gpt2  [I, Ġlove, ĠN, LP, !, ĠVisit, Ġhttps, ://, exa...   \n",
            "\n",
            "   Count   Time  \n",
            "0     12  0.000  \n",
            "1     18  0.000  \n",
            "2     14  0.014  \n",
            "3     26  0.394  \n",
            "4     25  0.518  \n"
          ]
        }
      ]
    }
  ]
}